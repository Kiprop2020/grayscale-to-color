{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"isu8mbCCm2py"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Input\n","from tensorflow.keras.utils import img_to_array, array_to_img\n","from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, ReLU, BatchNormalization, Concatenate\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import os\n","from PIL import Image"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyC2gWNZkpmR","executionInfo":{"status":"ok","timestamp":1677157707223,"user_tz":-180,"elapsed":20199,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"15c3d02a-5d78-4b41-871f-b9faf9276127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_bCYFr7KZed"},"outputs":[],"source":["train_images_folder = '/content/drive/MyDrive/coco_test2017'\n","val_images_folder = '/content/drive/MyDrive/coco_test2017/val'\n","model_path = '/content/drive/MyDrive/image colorization/grayscale_to_rgb_model_coco_test_2017.keras'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0Pl6h-_iXnS"},"outputs":[],"source":["layer_filters = 32\n","image_size = 512\n","\n","# Inputs\n","model_inputs = Input(shape=(image_size, image_size, 1))\n","x = model_inputs\n","\n","# Convolutional blocks\n","x = Conv2D(filters=layer_filters * 1, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_1 = x\n","\n","x = Conv2D(filters=layer_filters * 2, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_2 = x\n","\n","x = Conv2D(filters=layer_filters * 3, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_3 = x\n","\n","x = Conv2D(filters=layer_filters * 4, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_4 = x\n","\n","x = Conv2D(filters=layer_filters * 5, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_5 = x\n","\n","x = Conv2D(filters=layer_filters * 6, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_6 = x\n","\n","x = Conv2D(filters=layer_filters * 7, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","conv_block_7 = x\n","\n","# Transpose convolution blocks\n","\n","x = Conv2DTranspose(filters=layer_filters * 7, kernel_size=3, strides=1, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_1 = x\n","x = Concatenate()([convt_block_1, conv_block_7])\n","\n","x = Conv2DTranspose(filters=layer_filters * 7, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_2 = x\n","x = Concatenate()([convt_block_2, conv_block_6])\n","\n","x = Conv2DTranspose(filters=layer_filters * 6, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_3 = x\n","x = Concatenate()([convt_block_3, conv_block_5])\n","\n","x = Conv2DTranspose(filters=layer_filters * 5, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_4 = x\n","x = Concatenate()([convt_block_4, conv_block_4])\n","\n","x = Conv2DTranspose(filters=layer_filters * 4, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_5 = x\n","x = Concatenate()([convt_block_5, conv_block_3])\n","\n","x = Conv2DTranspose(filters=layer_filters * 3, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_6 = x\n","x = Concatenate()([convt_block_6, conv_block_2])\n","\n","x = Conv2DTranspose(filters=layer_filters * 2, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_7 = x\n","x = Concatenate()([convt_block_7, conv_block_1])\n","\n","x = Conv2DTranspose(filters=layer_filters * 1, kernel_size=3, strides=2, padding='same')(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)\n","convt_block_8 = x\n","x = Concatenate()([convt_block_8, model_inputs])\n","\n","model_outputs = Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same')(x)\n","model = tf.keras.Model(model_inputs, model_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bj_pI63tTePO"},"outputs":[],"source":["loss = tf.keras.losses.MeanSquaredError()\n","optimizer = tf.keras.optimizers.Adam()\n","metrics = ['accuracy']\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='loss', save_best_only=True)\n","\n","model = tf.keras.Model(model_inputs, model_outputs)\n","model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"4qiW7m53oS2I"},"source":["To do:\n","* Read images from directory\n","* Reshape images to be 512 * 512 * 1\n","* Normal preprocessing - batching, \n","* Train the model\n","* Make predictions\n","* Reshape the predictions to the initial shape"]},{"cell_type":"code","source":["# from keras.utils.image_dataset import image_dataset_from_directory\n","rgb_datagen = ImageDataGenerator(rescale=1./255)\n","grayscale_datagen = ImageDataGenerator(rescale=1./255)\n","\n","rgb_generator = rgb_datagen.flow_from_directory(\n","    train_images_folder,\n","    target_size=(512, 512),\n","    color_mode='rgb',\n","    batch_size=32,\n","    shuffle=False,\n","    class_mode=None\n",")\n","\n","grayscale_generator = grayscale_datagen.flow_from_directory(\n","    train_images_folder,\n","    target_size=(512, 512),\n","    color_mode='grayscale',\n","    batch_size=32,\n","    shuffle=False,\n","    class_mode=None\n",")\n","\n","train_generator = zip(grayscale_generator, rgb_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsZ-S30UWtDn","executionInfo":{"status":"ok","timestamp":1676552417839,"user_tz":-180,"elapsed":3079,"user":{"displayName":"Peter Kiprop Kurui","userId":"06533757863623377831"}},"outputId":"d4c6c4ba-b454-4c38-fa07-eb2ce9d2e73f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 40669 images belonging to 1 classes.\n","Found 40669 images belonging to 1 classes.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tlb5H2MJKLq_"},"outputs":[],"source":["# seed = 1024\n","# batch_size = 32\n","\n","# train_images = tf.keras.utils.image_dataset_from_directory(\n","#     train_images_folder,\n","#     labels=None,\n","#     label_mode=None,\n","#     class_names=None,\n","#     color_mode='rgb',\n","#     batch_size=None,\n","#     image_size=(512, 512),\n","#     shuffle=False,\n","#     seed=seed,\n","#     validation_split=None,\n","#     subset=None,\n","#     interpolation='bilinear',\n","#     follow_links=False,\n","#     crop_to_aspect_ratio=False\n","# )\n","\n","# val_images = tf.keras.utils.image_dataset_from_directory(\n","#     val_images_folder,\n","#     labels=None,\n","#     label_mode=None,\n","#     class_names=None,\n","#     color_mode='rgb',\n","#     batch_size=None,\n","#     image_size=(512, 512),\n","#     shuffle=False,\n","#     seed=seed,\n","#     validation_split=None,\n","#     subset=None,\n","#     interpolation='bilinear',\n","#     follow_links=False,\n","#     crop_to_aspect_ratio=False\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyMPWVmKQmQP"},"outputs":[],"source":["# AUTOTUNE = tf.data.AUTOTUNE\n","# def normalize_images(image):\n","#     return tf.cast(tf.image.rgb_to_grayscale(image), tf.float32)/255., tf.cast(image, tf.float32)/255.\n","\n","# def preprocess_dataset(dataset):\n","#     return dataset.shuffle(seed).batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)\n","\n","# train_images_normalized = train_images.map(normalize_images, num_parallel_calls=AUTOTUNE)\n","# preprocessed_train_dataset = preprocess_dataset(train_images_normalized)\n","\n","# val_images_normalized = val_images.map(normalize_images, num_parallel_calls=AUTOTUNE)\n","# preprocessed_val_dataset = preprocess_dataset(val_images_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gb2ZqoiUTT9D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb632ff1-8ae9-4583-8858-499a444a29b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-d0720aad54a0>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  history = model.fit_generator(train_generator, epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[cp_callback])\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1270/1270 [==============================] - 13328s 10s/step - loss: 0.0104 - accuracy: 0.4494\n","Epoch 2/100\n","1270/1270 [==============================] - 1368s 1s/step - loss: 0.0083 - accuracy: 0.4770\n","Epoch 3/100\n","1270/1270 [==============================] - 1357s 1s/step - loss: 0.0076 - accuracy: 0.5160\n","Epoch 4/100\n","1270/1270 [==============================] - 1292s 1s/step - loss: 0.0071 - accuracy: 0.5396\n","Epoch 5/100\n","1270/1270 [==============================] - 1272s 1s/step - loss: 0.0069 - accuracy: 0.5492\n","Epoch 6/100\n","1270/1270 [==============================] - 1267s 997ms/step - loss: 0.0067 - accuracy: 0.5557\n","Epoch 7/100\n","1270/1270 [==============================] - 1268s 998ms/step - loss: 0.0066 - accuracy: 0.5618\n","Epoch 8/100\n"," 792/1270 [=================>............] - ETA: 7:55 - loss: 0.0065 - accuracy: 0.5649"]}],"source":["steps_per_epoch = 40669/32\n","history = model.fit_generator(train_generator, epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[cp_callback])"]},{"cell_type":"markdown","metadata":{"id":"vIJ5ZWD5D4wx"},"source":["Test data:\n","* Preprocess - batch_size of 1\n","* Have the real rgb images to compare with the predictions"]},{"cell_type":"code","source":["ken = np.array(2, 5)"],"metadata":{"id":"osCzXmoHa_eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-x8hmRgTD2Pz"},"outputs":[],"source":["# def normalize_val_images_grayscale(image):\n","#     return tf.cast(tf.image.rgb_to_grayscale(image), tf.float32)/255.\n","\n","# def normalize_val_images_rgb(image):\n","#     return tf.cast(image, tf.float32)/255.\n","\n","# def preprocess_val_dataset(dataset):\n","#     return dataset.batch(1).prefetch(AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fy8dhnTwWGuP"},"outputs":[],"source":["# normalized_grayscale_val_images = val_images.map(normalize_val_images_grayscale, num_parallel_calls=AUTOTUNE)\n","# preprocessed_grayscale_val_dataset = preprocess_val_dataset(normalized_grayscale_val_images)\n","\n","# normalized_rgb_val_images = val_images.map(normalize_val_images_rgb, num_parallel_calls=AUTOTUNE)\n","# preprocessd_rgb_val_dataset = preprocess_val_dataset(normalized_rgb_val_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5B-0EYyFJdF5"},"outputs":[],"source":["# model = tf.keras.models.load_model(model_path)\n","# predictions = model.predict(preprocessed_grayscale_val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ck483FdYQ_pg"},"outputs":[],"source":["# actual_grayscale_arrays = [np.array(image) for image in normalized_grayscale_val_images]\n","# actual_rgb_arrays = [np.array(image) for image in normalized_rgb_val_images]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHjjxvaHMXwO"},"outputs":[],"source":["# rand_int = int(np.random.randint(low=0, high=500, size=1))\n","# plt.imshow(array_to_img(actual_grayscale_arrays[rand_int]), cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y68IzfUULeJo"},"outputs":[],"source":["# plt.imshow(array_to_img(predictions[rand_int]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO3gwf2nOrLL"},"outputs":[],"source":["# plt.imshow(array_to_img(actual_rgb_arrays[rand_int]))"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"4d68ad159a71ad314307ceabd6cda7798f1e50a960966662c6e9782a39660170"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
